# â¤ï¸ **HeartBeat AI â€“ Cardiac Sound Classifier**

*A deep learningâ€“powered clinical decision support system for automated classification of cardiac (heart) sounds.*



HeartBeat AI analyzes short audio recordings of heartbeats and determines whether they contain **normal** or **abnormal** cardiac sound patterns. It uses mel-spectrograms and a custom-designed Convolutional Neural Network (CNN) trained on real-world heart sound datasets.

---

##  **Live Demo**

- ** Dashboard**: [https://heartbeat-ai-classifier.streamlit.app](https://heartbeat-ai-classifier.streamlit.app)
- ** API**: [https://heartbeat-ai-api.onrender.com](https://heartbeat-ai-api.onrender.com)
- **ğŸ“¹ Video Demo**: [Watch on YouTube](#) *(Add your link after recording)*


>  **Note**: API sleeps after 15 minutes on Render's free tier. First call may take ~30s to wake up.

---

##  **Project Overview**

Heart sound analysis is crucial for early detection of cardiovascular diseases. However, stethoscope interpretation requires specialist training and is often unavailable in low-resource settings.

**HeartBeat AI addresses this by:**
- Providing fast & accurate screening (85.5% accuracy)
- Supporting multiple audio formats (WAV, MP3, FLAC, OGG)
- Offering an accessible web dashboard
-  Enabling integration through REST APIs
-  Handling batch classification

The system uses **mel-spectrograms** to represent cardiac audio and trains a **CNN** to classify normal vs abnormal heart sounds.

---

##  **Core Features**

###  Audio Classification
- Upload WAV, MP3, FLAC, or OGG files
- Real-time prediction with confidence scores
- Support for 5-second cardiac sound clips

### Batch Processing
- Process multiple recordings simultaneously
- Structured JSON results with per-file status
- Error handling for invalid files

###  Analytics & Visualizations
- Class distribution charts
- Training/validation curves
- Confusion matrix & ROC curves
- Real-time system metrics
- Prediction history tracking

###  API-Driven Architecture
Fully documented REST API with:
- `POST /predict` - Single audio classification
- `POST /batch-predict` - Multiple files
- `GET /health` - System status
- `GET /uptime` - Server metrics
- `GET /metrics` - Model performance

###  CNN Model
- **2.45M parameters**
- **128-band mel spectrogram** input (128Ã—79Ã—1)
- **3 convolutional blocks** with BatchNorm & Dropout
- **Dense layers** for binary classification
- **Trained on 3,240 samples** with 80/20 split

---

##  **Project Structure**

```
cardiac-sound-classifier/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ api.py              # FastAPI application
â”‚   â”œâ”€â”€ config.py           # Global settings
â”‚   â”œâ”€â”€ train.py            # CNN training script
â”‚   â”œâ”€â”€ preprocessing.py    # Audio preprocessing
â”‚   â”œâ”€â”€ data_loader.py      # Dataset utilities
â”‚   â”œâ”€â”€ model.py            # Model architecture
â”‚   â””â”€â”€ prediction.py       # Inference utilities
â”‚
â”œâ”€â”€ frontend/
â”‚   â””â”€â”€ app.py              # Streamlit dashboard
â”‚
â”œâ”€â”€ models/
â”‚   â””â”€â”€ cardiac_cnn_model.h5   # Trained model (2.45M params)
â”‚
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ train/              # Training data (2,592 samples)
â”‚   â””â”€â”€ validation/         # Validation data (648 samples)
â”‚
â”œâ”€â”€ outputs/
â”‚   â”œâ”€â”€ class_distribution.png
â”‚   â”œâ”€â”€ confusion_matrix.png
â”‚   â”œâ”€â”€ training_history.png
â”‚   â”œâ”€â”€ roc_curve.png
â”‚   â””â”€â”€ mel_spectrogram_comparison.png
â”‚
â”œâ”€â”€ tools/
â”‚   â”œâ”€â”€ run_batch_test.py   # Batch testing utility
â”‚   â”œâ”€â”€ check_backend.py    # Backend health check
â”‚   â””â”€â”€ inspect_routes.py   # API route inspector
â”‚
â”œâ”€â”€ tests/
â”‚   â””â”€â”€ locustfile.py       # Load testing
â”‚
â”œâ”€â”€ notebook/
â”‚   â””â”€â”€ heartbeat_ai_eda.ipynb  # EDA & training
â”‚
â”œâ”€â”€ Dockerfile
â”œâ”€â”€ docker-compose.yml
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
```

---

##  **Dataset Summary**

**Total Samples**: 3,240 cardiac sound recordings

**Split Distribution**:
- **Training**: 2,592 samples (80%)
  - Normal: 2,060 (79.5%)
  - Abnormal: 532 (20.5%)
- **Validation**: 648 samples (20%)
  - Normal: 515 (79.5%)
  - Abnormal: 133 (20.5%)

**Class Balance**: The dataset has a natural class imbalance with 79.5% normal and 20.5% abnormal samples, reflecting real-world distribution of cardiac conditions.

---

##  **Feature Interpretation & Clinical Insights**

The analysis examined three key acoustic features that distinguish normal from abnormal cardiac sounds:

### 1. Frequency Band Energy Distribution

| Frequency Band | Normal (dB) | Abnormal (dB) | Î” | Clinical Interpretation |
|----------------|-------------|---------------|---|------------------------|
| **Low (0â€“42 Hz)** | -41.56 | -42.23 | -0.67 | Reduced fundamental cardiac tone |
| **Mid (42â€“85 Hz)** | -61.28 | -61.68 | -0.40 | Minimal discriminative power |
| **High (85â€“128 Hz)** | -78.98 | -79.41 | -0.43 | Loss of high-frequency harmonics |

** Clinical Insight**: Abnormal heart sounds exhibit reduced energy across all frequency bands, particularly in high frequencies. This aligns with medical knowledge that murmurs and abnormal sounds result from disrupted laminar blood flow, affecting the harmonic structure of cardiac sounds.

### 2. Temporal Energy Patterns

| Metric | Normal | Abnormal | Interpretation |
|--------|--------|----------|----------------|
| **Mean Energy** | 0.1044 | 0.1028 | Normal hearts produce stronger contractions |
| **Energy Variance** | 0.00496 | 0.00470 | Normal sounds show clearer S1/S2 cycles |
| **Energy Range** | 0.2216 | 0.2248 | Abnormal sounds have irregular peaks |

** Clinical Insight**: Normal heartbeats exhibit consistent energy patterns with clear peaks corresponding to the "lub-dub" sounds (S1 and S2). Abnormal recordings show irregular energy distribution, reflecting pathological conditions like valve regurgitation, stenosis, or septal defects.

### 3. Spectral Characteristics

| Feature | Normal | Abnormal | Interpretation |
|---------|--------|----------|----------------|
| **Spectral Centroid** | 106.72 Hz | 102.55 Hz | Abnormal sounds are "darker" (lower pitch) |
| **Spectral Bandwidth** | 151.09 Hz | 159.40 Hz | Wider frequency spread in abnormal sounds |
| **Zero-Crossing Rate** | 0.0314 | 0.0358 | Higher complexity in abnormal sounds |

** Clinical Insight**: Abnormal cardiac sounds are characterized by broader frequency dispersion and higher complexity. This reflects the acoustic physics of cardiovascular pathology: abnormal valve function or turbulent blood flow creates additional frequency components, resulting in murmurs, clicks, or gallops.

### Feature Integration

The CNN learns these patterns automatically from mel-spectrograms, acting as an advanced pattern recognition system that detects subtle acoustic signatures invisible to the human ear. The three features work synergistically:

1. **Frequency bands** â†’ Overall spectral energy profile
2. **Temporal patterns** â†’ Rhythmic structure of heartbeats
3. **Spectral characteristics** â†’ Acoustic complexity and quality

---

##  **Model Performance**

### Validation Metrics

| Metric | Value | Interpretation |
|--------|-------|----------------|
| **Accuracy** | 85.5% | Overall classification accuracy |
| **Precision (Normal)** | 91% | High confidence in normal predictions |
| **Recall (Normal)** | 91% | Excellent normal sound detection |
| **Precision (Abnormal)** | 65% | Moderate abnormal precision |
| **Recall (Abnormal)** | 64% | Decent abnormal detection rate |
| **F1 Score** | 0.644 | Balanced performance |
| **AUC-ROC** | 0.900 | Excellent class separation |
| **Training Time (CPU)** | ~6 min | Fast training on standard hardware |

### Confusion Matrix

|  | **Predicted Normal** | **Predicted Abnormal** |
|---|---------------------|----------------------|
| **Actual Normal** | 468 (91%) | 47 (9%) |
| **Actual Abnormal** | 48 (36%) | 85 (64%) |

### Performance Analysis

**Strengths:**
- âœ… High normal recall (91%) â†’ Excellent at confirming healthy sounds
- âœ… High AUC (0.90) â†’ Strong class separation capability
- âœ… Balanced performance despite class imbalance

**Areas for Improvement:**
-  Moderate abnormal precision (65%) â†’ Some false positives
- Abnormal recall (64%) could be higher for medical screening

**Clinical Significance:**
- In medical screening, higher **recall** for abnormal cases is prioritized (better to flag for review than miss)
- The model achieves clinically useful performance given the 20.5% class imbalance
- 91% normal recall means low false alarm rate for healthy patients

---

##  **CNN Architecture**

```
Model: "sequential"
_________________________________________________________________
Layer (type)                Output Shape              Param #   
=================================================================
conv2d (Conv2D)             (None, 128, 79, 32)       320       
batch_normalization         (None, 128, 79, 32)       128       
max_pooling2d               (None, 64, 39, 32)        0         
dropout                     (None, 64, 39, 32)        0         

conv2d_1 (Conv2D)           (None, 64, 39, 64)        18,496    
batch_normalization_1       (None, 64, 39, 64)        256       
max_pooling2d_1             (None, 32, 19, 64)        0         
dropout_1                   (None, 32, 19, 64)        0         

conv2d_2 (Conv2D)           (None, 32, 19, 128)       73,856    
batch_normalization_2       (None, 32, 19, 128)       512       
max_pooling2d_2             (None, 16, 9, 128)        0         
dropout_2                   (None, 16, 9, 128)        0         

flatten                     (None, 18432)             0         
dense                       (None, 128)               2,359,424 
batch_normalization_3       (None, 128)               512       
dropout_3                   (None, 128)               0         
dense_1 (Dense)             (None, 1)                 129       
=================================================================
Total params: 2,453,633 (9.36 MB)
Trainable params: 2,452,929 (9.36 MB)
Non-trainable params: 704 (2.75 KB)
```

**Key Design Decisions:**
- **3 Conv blocks**: Progressive feature extraction (32â†’64â†’128 filters)
- **BatchNorm**: Stabilizes training and improves convergence
- **Dropout (0.25, 0.5)**: Prevents overfitting
- **Sigmoid output**: Binary classification probabilities

---

##  **Installation & Setup**

### Prerequisites
- Python 3.11+
- Git with Git LFS (for model file)
- 4GB RAM minimum
- TensorFlow-compatible system

### Quick Start

```bash
# 1. Clone repository
git clone https://github.com/RELEBOHILE-PHEKO/cardiac-sound-classifier.git
cd cardiac-sound-classifier

# 2. Create virtual environment
python -m venv .venv

# Windows
.venv\Scripts\activate

# Linux/Mac
source .venv/bin/activate

# 3. Install dependencies
pip install -r requirements.txt

# 4. Pull model file
git lfs pull

# 5. Start API server
uvicorn src.api:app --host 0.0.0.0 --port 8000 --reload

# 6. Start dashboard (new terminal)
streamlit run frontend/app.py --server.port 8501
```

Access dashboard at: **http://localhost:8501**

---

## ğŸ“¡ **API Documentation**

### Base URLs
- **Local**: `http://localhost:8000`
- **Production**: `https://heartbeat-ai-api.onrender.com`

### Endpoints

#### ğŸŸ¢ Health Check
```http
GET /health
```

**Response:**
```json
{
  "status": "ok",
  "model_loaded": true
}
```

#### ğŸŸ¢ Get Uptime
```http
GET /uptime
```

**Response:**
```json
{
  "uptime_seconds": 3600.5,
  "started_at": "2025-11-23T10:00:00Z"
}
```

#### ğŸŸ¢ Single Prediction
```http
POST /predict
Content-Type: multipart/form-data

file: <audio_file.wav>
```

**Response:**
```json
{
  "predicted_class": "abnormal",
  "confidence": 0.7983,
  "probability_normal": 0.2017,
  "probability_abnormal": 0.7983,
  "file_name": "heartbeat.wav",
  "timestamp": "2025-11-23T16:30:00Z"
}
```

#### ğŸŸ¢ Batch Prediction
```http
POST /batch-predict
Content-Type: multipart/form-data

files: <audio_file_1.wav>
files: <audio_file_2.wav>
...
```

**Response:**
```json
{
  "results": [
    {
      "predicted_class": "normal",
      "confidence": 0.8234,
      "probability_normal": 0.8234,
      "probability_abnormal": 0.1766,
      "file_name": "sample1.wav",
      "status": "success",
      "timestamp": "2025-11-23T16:30:00Z"
    },
    {
      "predicted_class": "abnormal",
      "confidence": 0.6521,
      "file_name": "sample2.wav",
      "status": "success"
    }
  ],
  "total": 2
}
```

#### ğŸŸ¢ Get Metrics
```http
GET /metrics
```

**Response:**
```json
{
  "accuracy": 0.855,
  "predictions_served": 1247
}
```

### Python Usage Example

```python
import requests

# Single prediction
url = "https://heartbeat-ai-api.onrender.com/predict"
with open("heartbeat.wav", "rb") as f:
    files = {"file": f}
    response = requests.post(url, files=files)
    print(response.json())

# Batch prediction
url = "https://heartbeat-ai-api.onrender.com/batch-predict"
files = [
    ("files", open("sample1.wav", "rb")),
    ("files", open("sample2.wav", "rb")),
    ("files", open("sample3.wav", "rb"))
]
response = requests.post(url, files=files)
print(response.json())
```

---

## ğŸ“ **Training Your Own Model**

### 1. Prepare Dataset

```
data/
â”œâ”€â”€ train/
â”‚   â”œâ”€â”€ normal/      # Normal heartbeat WAV files
â”‚   â””â”€â”€ abnormal/    # Abnormal heartbeat WAV files
â””â”€â”€ validation/
    â”œâ”€â”€ normal/
    â””â”€â”€ abnormal/
```

### 2. Configure Training

Edit `src/config.py`:
```python
EPOCHS = 20
BATCH_SIZE = 16
LEARNING_RATE = 0.001
TARGET_SAMPLE_RATE = 4000
SEGMENT_SECONDS = 5.0
```

### 3. Run Training

```bash
python src/train.py
```

Model will be saved to `models/cardiac_cnn_model.h5`

### 4. Evaluate

The notebook `notebook/heartbeat_ai_eda.ipynb` contains comprehensive evaluation code including:
- Confusion matrix
- ROC curve
- Precision-recall curves
- Feature analysis

---

## ğŸ§ª **Testing**

### Unit Tests
```bash
pytest tests/
```

### Batch Testing
```bash
python tools/run_batch_test.py --url http://localhost:8000 --folder data/validation --limit 10
```

### Load Testing (Locust)

**Results from local testing:**
- âš¡ Peak throughput: ~50 requests/second
- â±ï¸ Median response: 10-12ms
- ğŸ“Š Average response: 12-32ms
- âœ… Zero failures in 4,200+ requests

**Run your own load test:**
```bash
locust -f tests/locustfile.py --headless \
  -u 100 -r 50 -t 2m \
  --host http://localhost:8000 \
  --html outputs/locust_report.html
```

---

##  **Docker Deployment**

### Build and Run

```bash
# Build containers
docker-compose build

# Start services
docker-compose up

# Run in background
docker-compose up -d
```

### Services

- **API**: http://localhost:8000
- **Dashboard**: http://localhost:8501

---

## â˜ï¸ **Cloud Deployment**

### Backend (Render)

1. Create account on [Render.com](https://render.com)
2. New Web Service â†’ Connect GitHub repository
3. Configure:
   - **Build Command**: `pip install -r requirements.txt`
   - **Start Command**: `uvicorn src.api:app --host 0.0.0.0 --port $PORT`
   - **Environment**: Python 3.11
   - **Instance Type**: Free tier

### Frontend (Streamlit Cloud)

1. Go to [share.streamlit.io](https://share.streamlit.io)
2. Deploy from GitHub repository
3. Set **Main file**: `frontend/app.py`
4. Add secrets (optional):
```toml
API_BASE_URL = "https://your-api-url.onrender.com"
```

---

## ğŸ› ï¸ **Troubleshooting**

### Model Not Loading
```bash
# Ensure Git LFS is installed
git lfs install

# Pull model file
git lfs pull

# Verify model exists
ls -lh models/cardiac_cnn_model.h5
```

### API Connection Issues
- Check if Render service is awake (free tier sleeps after 15min)
- Verify API_BASE_URL in `frontend/app.py`
- Review Render logs for errors

### Audio Format Errors
- Supported: WAV, MP3, FLAC, OGG
- Max size: 200MB per file
- Sample rate: Will be resampled to 4000Hz
- Duration: Will be padded/trimmed to 5 seconds

### TensorFlow Errors
```bash
# For CPU-only systems
pip install tensorflow-cpu

# For GPU systems
pip install tensorflow-gpu
```

---

##  **Clinical Impact & Future Work**

### Current Capabilities
- âœ… Binary classification (normal vs abnormal)
- âœ… 85.5% validation accuracy
- âœ… 0.90 AUC-ROC score
- âœ… Real-time predictions (<50ms)
- âœ… Batch processing support

### Limitations
- âš ï¸ Binary classification only (not specific diagnoses)
- âš ï¸ Requires high-quality audio recordings
- âš ï¸ Screening tool, not diagnostic replacement
- âš ï¸ Class imbalance in training data

### Future Enhancements
- ğŸ”® Multi-class classification (specific cardiac conditions)
- ğŸ”® Attention mechanisms for explainability
- ğŸ”® Ensemble models for improved accuracy
- ğŸ”® Mobile app development
- ğŸ”® Real-time audio streaming
- ğŸ”® Integration with wearable devices
- ğŸ”® Larger, more diverse datasets

### Clinical Applications
1. **Primary Care Screening**: Pre-assessment before cardiologist referral
2. **Telemedicine**: Remote cardiac monitoring
3. **Resource-Limited Settings**: Accessible diagnostic tool
4. **Medical Education**: Training aid for medical students
5. **Continuous Monitoring**: Integration with smart health devices

---

## ğŸ“š **References & Acknowledgments**

### Datasets
- PhysioNet/CinC Challenge 2016: Heart Sound Database
- [Dataset Link](https://physionet.org/content/challenge-2016/1.0.0/)

### Technologies
- **TensorFlow/Keras**: Deep learning framework
- **FastAPI**: High-performance API framework
- **Streamlit**: Interactive web dashboard
- **LibROSA**: Audio analysis library
- **Scikit-learn**: Machine learning utilities

### Research Papers
1. Liu, C., et al. (2016). "An open access database for the evaluation of heart sound algorithms"
2. Potes, C., et al. (2016). "Ensemble of feature-based and deep learning-based classifiers for detection of abnormal heart sounds"

---

##  **Contributing**

Contributions are welcome! Please:

1. Fork the repository
2. Create a feature branch (`git checkout -b feature/AmazingFeature`)
3. Commit changes (`git commit -m 'Add AmazingFeature'`)
4. Push to branch (`git push origin feature/AmazingFeature`)
5. Open a Pull Request

### Development Guidelines
- Follow PEP 8 style guide
- Add unit tests for new features
- Update documentation
- Ensure all tests pass before submitting



---

## ğŸ‘¤ **Author**

**Relebohile Pheko**
- ğŸ“§ Email: [relebohilepheko1@gmail.com](mailto:relebohilepheko1@gmail.com)
- ğŸ’¼ LinkedIn: [Your LinkedIn](#)
- ğŸ™ GitHub: [@RELEBOHILE-PHEKO](https://github.com/RELEBOHILE-PHEKO)

---


---

## ğŸ™ **Acknowledgments**

Special thanks to:
- PhysioNet for providing the heart sound dataset
- The TensorFlow and Keras teams for the deep learning framework
- The FastAPI and Streamlit communities for excellent tools
- All contributors and testers who helped improve this project

---
