{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "46d58fd2",
   "metadata": {},
   "source": [
    "# HeartBeat AI Exploratory Data Analysis\n",
    "\n",
    "End-to-end analysis of cardiac and respiratory auscultation signals covering data inspection, preprocessing, baseline modeling, evaluation, and interpretability.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17f3a071",
   "metadata": {},
   "source": [
    "## Notebook Roadmap\n",
    "- Dataset structure overview & sanity checks\n",
    "- Audio preprocessing experiments (waveform + log-mel)\n",
    "- Baseline HeartBeat ensemble training runs\n",
    "- Evaluation with multi-metric reporting & confusion matrix\n",
    "- Prediction demos and Grad-CAM-style insights\n",
    "- Feature interpretation: frequency bands, temporal textures, cadence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "aed52498",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "try:\n",
    "    ROOT = Path(__file__).resolve().parents[1]\n",
    "except NameError:\n",
    "    ROOT = Path.cwd().parents[0]\n",
    "\n",
    "if str(ROOT) not in sys.path:\n",
    "    sys.path.append(str(ROOT))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "37ccb534",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[16]\u001b[39m\u001b[32m, line 15\u001b[39m\n\u001b[32m     12\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mIPython\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdisplay\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Audio, Image, display\n\u001b[32m     14\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msrc\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpreprocessing\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m AudioPreprocessor\n\u001b[32m---> \u001b[39m\u001b[32m15\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msrc\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mtrain\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m train_pipeline\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\PC\\cardiac-sound-classifier\\src\\train.py:13\u001b[39m\n\u001b[32m     11\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnp\u001b[39;00m\n\u001b[32m     12\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mseaborn\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msns\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m13\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtensorflow\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtf\u001b[39;00m\n\u001b[32m     14\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msklearn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmetrics\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m classification_report, confusion_matrix\n\u001b[32m     16\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msrc\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mconfig\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m settings\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'tensorflow'"
     ]
    }
   ],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import json\n",
    "from pathlib import Path\n",
    "from typing import Dict\n",
    "\n",
    "import librosa\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from IPython.display import Audio, Image, display\n",
    "\n",
    "from src.preprocessing import AudioPreprocessor\n",
    "from src.train import train_pipeline\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "496d1a4c",
   "metadata": {},
   "source": [
    "## 1. Data Loading & Exploration\n",
    "We mirror the required repository structure (`data/train/<class>` and `data/test/<class>`) and build a manifest to inspect class balance, durations, and sampling rates.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e49353b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = Path(\"../data/train\")\n",
    "CLASSES = [\n",
    "    \"normal_heart\",\n",
    "    \"murmur\",\n",
    "    \"extrasystole\",\n",
    "    \"normal_resp\",\n",
    "    \"wheeze\",\n",
    "    \"crackle\",\n",
    "]\n",
    "\n",
    "\n",
    "def build_manifest(root: Path) -> pd.DataFrame:\n",
    "    records = []\n",
    "    for label in CLASSES:\n",
    "        class_dir = root / label\n",
    "        if not class_dir.exists():\n",
    "            continue\n",
    "        for audio_path in class_dir.rglob(\"*.wav\"):\n",
    "            try:\n",
    "                y, sr = librosa.load(audio_path.as_posix(), sr=None, mono=True)\n",
    "                duration = len(y) / sr\n",
    "            except Exception:\n",
    "                duration = np.nan\n",
    "                sr = np.nan\n",
    "            records.append({\"path\": str(audio_path), \"label\": label, \"duration\": duration, \"sr\": sr})\n",
    "    return pd.DataFrame(records)\n",
    "\n",
    "manifest = build_manifest(DATA_DIR)\n",
    "manifest.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61ac956c",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not manifest.empty:\n",
    "    plt.figure(figsize=(8, 4))\n",
    "    sns.countplot(data=manifest, x=\"label\", order=CLASSES)\n",
    "    plt.xticks(rotation=30, ha=\"right\")\n",
    "    plt.title(\"Class Distribution - Training Set\")\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"Manifest is empty. Populate data/train before running analysis.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ee1a83a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preview_sample(label: str):\n",
    "    subset = manifest[manifest.label == label]\n",
    "    if subset.empty:\n",
    "        print(f\"No samples for {label}.\")\n",
    "        return\n",
    "    sample_path = Path(subset.sample(1, random_state=0).iloc[0].path)\n",
    "    y, sr = librosa.load(sample_path.as_posix(), sr=None)\n",
    "    display(Audio(y, rate=sr))\n",
    "    plt.figure(figsize=(10, 2))\n",
    "    plt.plot(np.linspace(0, len(y) / sr, len(y)), y)\n",
    "    plt.title(f\"Waveform preview: {label}\")\n",
    "    plt.show()\n",
    "\n",
    "# preview_sample(\"normal_heart\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3d2ceeb",
   "metadata": {},
   "source": [
    "## 2. Data Preprocessing Experiments\n",
    "Evaluate waveform normalization, fixed-length segmentation, mel-spectrogram creation, and augmentation techniques leveraged by `AudioPreprocessor`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5c70fd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessor = AudioPreprocessor(\n",
    "    target_sample_rate=4000,\n",
    "    segment_seconds=5.0,\n",
    "    n_mels=128,\n",
    ")\n",
    "\n",
    "if not manifest.empty:\n",
    "    example_path = Path(manifest.iloc[0].path)\n",
    "    waveform = preprocessor.load_waveform(example_path)\n",
    "    mel = preprocessor.to_mel_spectrogram(waveform)\n",
    "\n",
    "    fig, ax = plt.subplots(1, 2, figsize=(12, 3))\n",
    "    ax[0].plot(waveform)\n",
    "    ax[0].set_title(\"Normalized Waveform\")\n",
    "    sns.heatmap(mel[..., 0], ax=ax[1], cmap=\"magma\")\n",
    "    ax[1].set_title(\"Log-Mel Spectrogram\")\n",
    "    plt.tight_layout()\n",
    "else:\n",
    "    print(\"Preprocessing demo awaiting audio files.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "542a0a6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not manifest.empty:\n",
    "    aug_wave, aug_meta = preprocessor.augment(waveform)\n",
    "    plt.figure(figsize=(10, 2))\n",
    "    plt.plot(aug_wave)\n",
    "    plt.title(f\"Augmented waveform (noise={aug_meta['noise_factor']:.3f}, shift={aug_meta['time_shift']})\")\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "553d39a4",
   "metadata": {},
   "source": [
    "## 3. Baseline Training Runs\n",
    "The training routine in `src/train.py` orchestrates waveform/spectrogram preparation, model creation, callbacks, evaluation, and artifact persistence.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed3df39f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment when data directories contain audio files.\n",
    "# results = train_pipeline(Path(\"../data\"), epochs=20, batch_size=32)\n",
    "# results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6da7215",
   "metadata": {},
   "source": [
    "## 4. Model Evaluation & Diagnostics\n",
    "We track accuracy, precision/recall/F1, confusion matrix, and training curves. Outputs are persisted to `outputs/` and `monitoring/metrics.json` for the dashboard/API.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11935a61",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_file = Path(\"../monitoring/metrics.json\")\n",
    "if metrics_file.exists():\n",
    "    metrics: Dict[str, object] = json.loads(metrics_file.read_text())\n",
    "    metrics\n",
    "else:\n",
    "    print(\"Run training to generate metrics.json\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daa92a99",
   "metadata": {},
   "outputs": [],
   "source": [
    "conf_plot = Path(\"../outputs/confusion_matrix.png\")\n",
    "if conf_plot.exists():\n",
    "    display(Image(filename=conf_plot))\n",
    "else:\n",
    "    print(\"Confusion matrix plot pending training run.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7cc2e80",
   "metadata": {},
   "source": [
    "## 5. Prediction Examples & Grad-CAM\n",
    "After training, we will load the persisted model through `HeartbeatPredictor` to demonstrate single/batch inference, confidence spreads, and Grad-CAM overlays on spectrograms.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c57653c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from src.prediction import HeartbeatPredictor\n",
    "# predictor = HeartbeatPredictor(model_path=Path(\"../models/heartbeat_model.h5\"))\n",
    "# predictor.load()\n",
    "# predictor.predict(Path(\"../data/test/normal_heart/example.wav\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6688b97f",
   "metadata": {},
   "source": [
    "## 6. Feature Interpretation\n",
    "We will interpret three clinically relevant feature families:\n",
    "1. **Frequency bands** (low-frequency murmurs vs. high-frequency wheezes) using mel-bin statistics.\n",
    "2. **Temporal textures** with short-time energy and spectral flux to highlight extra sounds.\n",
    "3. **Cadence/shape** via autocorrelation and envelope tracking to monitor arrhythmias.\n",
    "\n",
    "Each subsection will include quantitative plots + text summaries connecting trends to medical reasoning.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94d9b297",
   "metadata": {},
   "outputs": [],
   "source": [
    "def frequency_band_summary(df: pd.DataFrame):\n",
    "    rows = []\n",
    "    for _, row in df.iterrows():\n",
    "        y, sr = librosa.load(row.path, sr=4000)\n",
    "        mel = librosa.feature.melspectrogram(y=y, sr=sr, n_mels=64)\n",
    "        bands = mel.mean(axis=1)\n",
    "        rows.append({\"label\": row.label, \"low_band\": bands[:16].mean(), \"mid_band\": bands[16:32].mean(), \"high_band\": bands[32:].mean()})\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "# band_stats = frequency_band_summary(manifest)\n",
    "# band_stats.groupby(\"label\").mean()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b53d3d42",
   "metadata": {},
   "source": [
    "> **Next steps:** populate the dataset, execute the cells above, capture plots/tables for the README and dashboard, and document insights gathered from each feature slice.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  },
  "title": "HeartBeat AI EDA"
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
